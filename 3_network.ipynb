{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_network.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMy0A+yH8fUvJFYKQTbgOIG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rusamentiaga/pytorch-test/blob/main/3_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUGxYfzVf7qC"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters \n",
        "input_size = 784\n",
        "hidden_size = 500\n",
        "num_classes = 10\n",
        "num_epochs = 5\n",
        "batch_size = 100\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KClWoRDMjkiV",
        "outputId": "3d61fcf9-f4d4-4612-ec88-5c031ea80680"
      },
      "source": [
        "device"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgIZPtEygC8h"
      },
      "source": [
        "# MNIST dataset \n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor(),  \n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
        "                                          train=False, \n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajErI-AhirYt",
        "outputId": "791a9e13-8323-4c89-8532-64ec2529c262"
      },
      "source": [
        "!ls data/MNIST/processed/"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test.pt  training.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "vx4CleuKo3D3",
        "outputId": "d9b42ce1-5c3c-4e5c-8aa1-5e5418226493"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for i, (images, labels) in enumerate(train_loader): \n",
        "  if i == 3:\n",
        "    plt.figure()\n",
        "    plt.imshow(images[0,0,:,:])\n",
        "    break\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIvULeUAfQyb",
        "outputId": "8d177890-b61e-4d61-fc5d-77ceb6302d66"
      },
      "source": [
        "\n",
        "\n",
        "# Fully connected neural network with one hidden layer\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "        # Move tensors to the configured device\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Step [100/600], Loss: 0.2681\n",
            "Epoch [1/5], Step [200/600], Loss: 0.1744\n",
            "Epoch [1/5], Step [300/600], Loss: 0.2154\n",
            "Epoch [1/5], Step [400/600], Loss: 0.2220\n",
            "Epoch [1/5], Step [500/600], Loss: 0.1067\n",
            "Epoch [1/5], Step [600/600], Loss: 0.1851\n",
            "Epoch [2/5], Step [100/600], Loss: 0.1124\n",
            "Epoch [2/5], Step [200/600], Loss: 0.0518\n",
            "Epoch [2/5], Step [300/600], Loss: 0.1242\n",
            "Epoch [2/5], Step [400/600], Loss: 0.1344\n",
            "Epoch [2/5], Step [500/600], Loss: 0.0455\n",
            "Epoch [2/5], Step [600/600], Loss: 0.0456\n",
            "Epoch [3/5], Step [100/600], Loss: 0.0618\n",
            "Epoch [3/5], Step [200/600], Loss: 0.0628\n",
            "Epoch [3/5], Step [300/600], Loss: 0.0719\n",
            "Epoch [3/5], Step [400/600], Loss: 0.0981\n",
            "Epoch [3/5], Step [500/600], Loss: 0.0338\n",
            "Epoch [3/5], Step [600/600], Loss: 0.0751\n",
            "Epoch [4/5], Step [100/600], Loss: 0.0272\n",
            "Epoch [4/5], Step [200/600], Loss: 0.0155\n",
            "Epoch [4/5], Step [300/600], Loss: 0.0514\n",
            "Epoch [4/5], Step [400/600], Loss: 0.0644\n",
            "Epoch [4/5], Step [500/600], Loss: 0.0825\n",
            "Epoch [4/5], Step [600/600], Loss: 0.0900\n",
            "Epoch [5/5], Step [100/600], Loss: 0.0332\n",
            "Epoch [5/5], Step [200/600], Loss: 0.0371\n",
            "Epoch [5/5], Step [300/600], Loss: 0.0608\n",
            "Epoch [5/5], Step [400/600], Loss: 0.0392\n",
            "Epoch [5/5], Step [500/600], Loss: 0.0928\n",
            "Epoch [5/5], Step [600/600], Loss: 0.0392\n",
            "Accuracy of the network on the 10000 test images: 98.11 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_mTHKuzf6CF",
        "outputId": "8bded56f-ef9c-4f00-cb4a-123e0ef4330b"
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.fc = nn.Linear(7*7*32, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "model = ConvNet(num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "# Test the model\n",
        "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
        "    "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Step [100/600], Loss: 0.1427\n",
            "Epoch [1/5], Step [200/600], Loss: 0.0750\n",
            "Epoch [1/5], Step [300/600], Loss: 0.0699\n",
            "Epoch [1/5], Step [400/600], Loss: 0.1148\n",
            "Epoch [1/5], Step [500/600], Loss: 0.0645\n",
            "Epoch [1/5], Step [600/600], Loss: 0.1749\n",
            "Epoch [2/5], Step [100/600], Loss: 0.0117\n",
            "Epoch [2/5], Step [200/600], Loss: 0.0344\n",
            "Epoch [2/5], Step [300/600], Loss: 0.0134\n",
            "Epoch [2/5], Step [400/600], Loss: 0.0494\n",
            "Epoch [2/5], Step [500/600], Loss: 0.0379\n",
            "Epoch [2/5], Step [600/600], Loss: 0.1414\n",
            "Epoch [3/5], Step [100/600], Loss: 0.1201\n",
            "Epoch [3/5], Step [200/600], Loss: 0.0426\n",
            "Epoch [3/5], Step [300/600], Loss: 0.0664\n",
            "Epoch [3/5], Step [400/600], Loss: 0.0194\n",
            "Epoch [3/5], Step [500/600], Loss: 0.0283\n",
            "Epoch [3/5], Step [600/600], Loss: 0.0327\n",
            "Epoch [4/5], Step [100/600], Loss: 0.0460\n",
            "Epoch [4/5], Step [200/600], Loss: 0.0803\n",
            "Epoch [4/5], Step [300/600], Loss: 0.0051\n",
            "Epoch [4/5], Step [400/600], Loss: 0.0250\n",
            "Epoch [4/5], Step [500/600], Loss: 0.0528\n",
            "Epoch [4/5], Step [600/600], Loss: 0.0131\n",
            "Epoch [5/5], Step [100/600], Loss: 0.0342\n",
            "Epoch [5/5], Step [200/600], Loss: 0.1190\n",
            "Epoch [5/5], Step [300/600], Loss: 0.0296\n",
            "Epoch [5/5], Step [400/600], Loss: 0.0204\n",
            "Epoch [5/5], Step [500/600], Loss: 0.0508\n",
            "Epoch [5/5], Step [600/600], Loss: 0.0038\n",
            "Test Accuracy of the model on the 10000 test images: 99.05 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}